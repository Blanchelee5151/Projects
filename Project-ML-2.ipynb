{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5bc04b3",
   "metadata": {},
   "source": [
    "# Project ML -2/2   Topic model on comments\n",
    "--Dan Hua Li\n",
    "\n",
    "Perform latent semantic analysis (LSA), non-negative matrix factorization (NMF), and latent Dirichlet allocation (LDA) using scikit-learn and Gensim on the ***Clean_comments dataset***. Set the number of topics = 10,3,5.  ( Try the top 10/20 words for each topic). Specific steps include:\n",
    "\n",
    "- a.\tscikit-learn for LSA, NMF, and LDA.\n",
    "- b.\tscikit-learn grid search for the best number of topics k for LDA, based on the perplexity score, with the grid of k = [2,3,4,5,6,7,8,10,12,14,16,18,20] (this search may take about 10-20 minutes).\n",
    "- c.\tGensim for LSI, NMF, and LDA.\n",
    "- d.\tGensim tqdm search for the best number of topics k for LDA, based on the c_v coherence score, with k ranging from 2 to 20 (consecutively).\n",
    "\n",
    "\n",
    "#### Import data 'clean_comments'(clean it in excel)\n",
    "clean Data: far more symbols need to be clean from Excel, especially:\n",
    "- https, youtube.com, github, .com (impact frequent words)\n",
    "- symbols from Emoticons (make insert data into python difficultly)\n",
    "- different language..\n",
    "- special symbols.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ccc4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "405d82f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10217 entries, 0 to 10216\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Reviews  10217 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 79.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thanks fucken.\\n\\nI decided to go into Tech in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello ken jee!!! I'm doing a graduation on Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thanks fuck for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great video!!! I started learning Python 8 mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Been watching hours ofucknow that it is not an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews\n",
       "0  Thanks fucken.\\n\\nI decided to go into Tech in...\n",
       "1  Hello ken jee!!! I'm doing a graduation on Com...\n",
       "2                                    Thanks fuck for\n",
       "3  Great video!!! I started learning Python 8 mon...\n",
       "4  Been watching hours ofucknow that it is not an..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install --upgrade gensim\n",
    "import pandas as pd\n",
    "# pd.options.display.max_rows = 100\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "textdf = pd.read_csv('Clean_comments-UTF8.csv',encoding='utf-8', sep=',')\n",
    "textdf.info()\n",
    "textdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5e0aa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '000' '00000001000' ... '…………' '›' '›\\x8d']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<10217x11886 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 112108 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def lemma_tokenizer(corpus): # a method to lemmatize corpus\n",
    "    corpus = ''.join([ch for ch in corpus if ch not in string.punctuation]) # remove punctuation\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('english') # use nltk's English stopwords list\n",
    "tf = CountVectorizer(tokenizer=lemma_tokenizer, stop_words=nltk_stopwords) # default lowercase\n",
    "tf_sparse = tf.fit_transform(textdf.Reviews)\n",
    "tf_dictionary = tf.get_feature_names_out()\n",
    "print(tf_dictionary)\n",
    "tf_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39c302c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '000' '00000001000' ... '…………' '›' '›\\x8d']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<10217x11884 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 111211 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stopwords.append('ha') # add 'ha' to stopword list for removal\n",
    "nltk_stopwords.append('hey')\n",
    "nltk_stopwords.append('wa')\n",
    "nltk_stopwords.append('hi')\n",
    "nltk_stopwords.append('Woo')\n",
    "tf = CountVectorizer(tokenizer=lemma_tokenizer, stop_words=nltk_stopwords) # default lowercase\n",
    "tf_sparse = tf.fit_transform(textdf.Reviews)\n",
    "tf_dictionary = tf.get_feature_names_out()\n",
    "print(tf_dictionary)\n",
    "tf_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aba60ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>000</th>\n",
       "      <th>00000001000</th>\n",
       "      <th>0015459375000001213</th>\n",
       "      <th>005</th>\n",
       "      <th>0050</th>\n",
       "      <th>00s</th>\n",
       "      <th>01</th>\n",
       "      <th>0100</th>\n",
       "      <th>0120</th>\n",
       "      <th>...</th>\n",
       "      <th>…</th>\n",
       "      <th>…watching</th>\n",
       "      <th>…¡</th>\n",
       "      <th>…¸</th>\n",
       "      <th>…û´</th>\n",
       "      <th>……</th>\n",
       "      <th>………</th>\n",
       "      <th>…………</th>\n",
       "      <th>›</th>\n",
       "      <th>›</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10212</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10213</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10214</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10215</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10216</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10217 rows × 11884 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  000  00000001000  0015459375000001213  005  0050  00s  01  0100   \n",
       "0      0    0            0                    0    0     0    0   0     0  \\\n",
       "1      0    0            0                    0    0     0    0   0     0   \n",
       "2      0    0            0                    0    0     0    0   0     0   \n",
       "3      0    0            0                    0    0     0    0   0     0   \n",
       "4      0    0            0                    0    0     0    0   0     0   \n",
       "...   ..  ...          ...                  ...  ...   ...  ...  ..   ...   \n",
       "10212  0    0            0                    0    0     0    0   0     0   \n",
       "10213  0    0            0                    0    0     0    0   0     0   \n",
       "10214  0    0            0                    0    0     0    0   0     0   \n",
       "10215  0    0            0                    0    0     0    0   0     0   \n",
       "10216  0    0            0                    0    0     0    0   0     0   \n",
       "\n",
       "       0120  ...  …  …watching  …¡  …¸  …û´  ……  ………  …………  ›  ›  \n",
       "0         0  ...  0          0   0    0    0   0    0     0  0   0  \n",
       "1         0  ...  0          0   0    0    0   0    0     0  0   0  \n",
       "2         0  ...  0          0   0    0    0   0    0     0  0   0  \n",
       "3         0  ...  0          0   0    0    0   0    0     0  0   0  \n",
       "4         0  ...  0          0   0    0    0   0    0     0  0   0  \n",
       "...     ...  ... ..        ...  ..  ...  ...  ..  ...   ... ..  ..  \n",
       "10212     0  ...  0          0   0    0    0   0    0     0  0   0  \n",
       "10213     0  ...  0          0   0    0    0   0    0     0  0   0  \n",
       "10214     0  ...  0          0   0    0    0   0    0     0  0   0  \n",
       "10215     0  ...  0          0   0    0    0   0    0     0  0   0  \n",
       "10216     0  ...  0          0   0    0    0   0    0     0  0   0  \n",
       "\n",
       "[10217 rows x 11884 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dense = tf_sparse.toarray() # convert sparse to dense matrix\n",
    "pd.DataFrame(tf_dense, columns=tf_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3762424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10217x11884 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 111211 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=lemma_tokenizer, stop_words=nltk_stopwords) # default lowercase\n",
    "tfidf_sparse = tfidf.fit_transform(textdf.Reviews)\n",
    "tfidf_dictionary = tfidf.get_feature_names_out()\n",
    "tfidf_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2dd808e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TruncatedSVD(n_components=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD(n_components=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TruncatedSVD(n_components=10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=10)\n",
    "lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9d9042e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10217, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_tf_topics = lsa.fit_transform(tf_sparse)\n",
    "lsa_tf_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b48ec3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 11884)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "620cfa52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA topics based on term-document matrix:\n",
      "Topic #0: data science ken video fuck im learning great would project\n",
      "Topic #1: data science scientist machine degree job computer analyst master field\n",
      "Topic #2: fuck im fucke ofuck hi get project learning much know\n",
      "Topic #3: video fuck im really learning like great get would ofuck\n",
      "Topic #4: im project really get learning would one like good fucke\n",
      "Topic #5: project thanks data scientist one great learning would question model\n",
      "Topic #6: science learning thanks project machine course computer master learn deep\n",
      "Topic #7: great learning would thank machine really one ken like content\n",
      "Topic #8: learning video thank much get like learn machine time fucke\n",
      "Topic #9: thanks learning great machine get lot really scientist fucke content\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print top terms for each topic\n",
    "def print_top_terms(model, vocabulary, n_top_terms):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([vocabulary[i]\n",
    "                             for i in topic.argsort()[:-n_top_terms - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "print('LSA topics based on term-document matrix:')\n",
    "print_top_terms(lsa, tf_dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e08a953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NMF(alpha_W=0.1, l1_ratio=0.5, n_components=10, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NMF</label><div class=\"sk-toggleable__content\"><pre>NMF(alpha_W=0.1, l1_ratio=0.5, n_components=10, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NMF(alpha_W=0.1, l1_ratio=0.5, n_components=10, random_state=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn for non-negative matrix factorization (NMF)\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=10, random_state=1, alpha_W=.1, l1_ratio=.5) # alpha_W and l1 related to regularization\n",
    "nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c097c612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.fit_transform(tf_sparse)\n",
    "# print('NMF topics based on term-document matrix:')\n",
    "# print_top_terms(nmf, tf_dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5139d03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.fit_transform(tfidf_sparse)\n",
    "# print('NMF topics based on tfidf matrix:')\n",
    "# print_top_terms(nmf, tfidf_dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2589c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn for latent Dirichlet allocation (LDA)\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=10, random_state=1, learning_method='online', learning_offset=50.)\n",
    "# lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8884e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topics based on term-document matrix:\n",
      "Topic #0: project use using like problem cool code deep idea model\n",
      "Topic #1: topic bro recommendation list got 0 day life habit ¸\n",
      "Topic #2: data science im ken fuck learning learn get course would\n",
      "Topic #3: resume excited ‚ wait cant email necessary already sent 25\n",
      "Topic #4: error getting tweet help line please first info element session\n",
      "Topic #5: video ken great thanks fuck thank really much love content\n",
      "Topic #6:  subscribed applying congrats „ 100k glad bring pas develop\n",
      "Topic #7: nice youre lol ” seems check people na bit gon\n",
      "Topic #8: comment company tell review like u 2 everyone ifuck see\n",
      "Topic #9: link page discord name api episode column conversation drop indian\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda.fit_transform(tf_sparse)\n",
    "print('LDA topics based on term-document matrix:')\n",
    "print_top_terms(lda, tf_dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ec556fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topics based on tfidf matrix:\n",
      "Topic #0: keyboard shirt eagerly john mouse kernel portfucken  snow de\n",
      "Topic #1: pizza pc papaya monitor funny commenting dope hot pick hardware\n",
      "Topic #2: honest discount annual lighting 365 titan super keen fuckenjee neural\n",
      "Topic #3: helpfucks datacamp x excited period ì haircut soo mic voice\n",
      "Topic #4: ¸ brother error thanx csv import wall sa object 115\n",
      "Topic #5: ken video data thanks fuck great science thank really im\n",
      "Topic #6:  10k plant biomedical keyword silver pycharm greate vomit woo\n",
      "Topic #7: thumbnail lmao gem absolute wink beautiful exact epic stormbreaker musk\n",
      "Topic #8: subscribed congrats road extra lol “ 100k sub remotely fast\n",
      "Topic #9: name green stufucken discord tea brilliant link secret congratulation algo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda.fit_transform(tfidf_sparse)\n",
    "print('LDA topics based on tfidf matrix:')\n",
    "print_top_terms(lda, tfidf_dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45a4aa0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=LatentDirichletAllocation(learning_method=&#x27;online&#x27;,\n",
       "                                                 learning_offset=50.0,\n",
       "                                                 random_state=1),\n",
       "             param_grid={&#x27;n_components&#x27;: [3, 5, 8, 10, 15, 18, 20]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=LatentDirichletAllocation(learning_method=&#x27;online&#x27;,\n",
       "                                                 learning_offset=50.0,\n",
       "                                                 random_state=1),\n",
       "             param_grid={&#x27;n_components&#x27;: [3, 5, 8, 10, 15, 18, 20]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          random_state=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          random_state=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=LatentDirichletAllocation(learning_method='online',\n",
       "                                                 learning_offset=50.0,\n",
       "                                                 random_state=1),\n",
       "             param_grid={'n_components': [3, 5, 8, 10, 15, 18, 20]})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn grid search for the best number of topics for LDA based on perplexity score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_components': [3,5,8,10,15,18,20]}\n",
    "lda = LatentDirichletAllocation(random_state=1, learning_method='online', learning_offset=50.)\n",
    "lda_grid = GridSearchCV(lda, param_grid)\n",
    "lda_grid.fit(tf_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71ec60b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model's params:  {'n_components': 3}\n",
      "Best log likelihood score:  -203295.05947136274\n",
      "Model perplexity:  1889.0721663769823\n",
      "-203295.05947136274 {'n_components': 3}\n",
      "-213036.8929452252 {'n_components': 5}\n",
      "-220867.81692879763 {'n_components': 8}\n",
      "-226860.10152153028 {'n_components': 10}\n",
      "-236008.06206140685 {'n_components': 15}\n",
      "-242457.09837598112 {'n_components': 18}\n",
      "-247687.1072977081 {'n_components': 20}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model's params: \", lda_grid.best_params_)\n",
    "print(\"Best log likelihood score: \", lda_grid.best_score_)\n",
    "print(\"Model perplexity: \", lda_grid.best_estimator_.perplexity(tf_sparse))\n",
    "cvresult = lda_grid.cv_results_\n",
    "for mean_test_score, params in zip(cvresult['mean_test_score'], cvresult['params']):\n",
    "    print(mean_test_score, params)\n",
    "# the best number of topics is  based on sklearn grid search, which does not looks good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acce93fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          n_components=3, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          n_components=3, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n",
       "                          n_components=3, random_state=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=3, random_state=1, learning_method='online', learning_offset=50.)\n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c1d0baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topics based on tfidf matrix:\n",
      "Topic #0: bro error name tweet helpfucks discord email line panda module call sound send file server keyboard brother pizza import sent\n",
      "Topic #1: ken video data thanks fuck great science thank really im project much good hi like would learning hey fucke love\n",
      "Topic #2: congrats subscribed 100k sub beast „ lol clock commenting congratulation notion gem super green template absolute lighting wall wink 50k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda.fit_transform(tfidf_sparse)\n",
    "print('LDA topics based on tfidf matrix:')\n",
    "print_top_terms(lda, tfidf_dictionary, 20) # 3 is terrible, i use component =20,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1840f",
   "metadata": {},
   "source": [
    "Wow, I am not sure this is what I need.\n",
    "### Continue search for best topic\n",
    "#### Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bdcd4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing time: 2.166 seconds.\n",
      "\n",
      "Tokenized documents:\n",
      " [['thanks', 'fucken', 'decided', 'go', 'tech', 'learning', 'web', 'development', 'learnt', 'bit', 'ofucknowledge', 'front', 'end', 'web', 'development', 'helpful', 'web', 'scraping'], ['hello', 'ken', 'jee', 'graduation', 'computer', 'science', 'really', 'keen', 'learn', 'data', 'science', 'fuck', 'honest', 'excitement', 'learning', 'data', 'science', 'get', 'top', 'notch', 'watching', 'video'], ['thanks', 'fuck'], ['great', 'video', 'started', 'learning', 'python', 'month', 'ago', 'quickly', 'became', 'interested', 'background', 'fucked', 'fuck', 'investment', 'said', 'im', 'considering', 'boot', 'camp', 'even', 'master', 'would', 'love', 'get', 'opinion', 'thanks'], ['watching', 'hour', 'ofucknow', 'easy', 'fucke', 'pinnacle', 'give'], ['hey', 'ken', 'almost', 'fucket', 'research', 'marketing', 'going', 'fuck', 'sure', 'compatibility', 'osx', 'fuck', 'good', 'idea', 'better', 'stick', 'zbooks', 'similar', 'window', 'laptop'], ['basically', 'modest', 'say', 'silver', 'bullet', 'get', 'good', 'role', 'sofuckely', 'analyst', 'scientist', 'ifuck', 'portfolio', 'professional', 'otherwise', 'intensively', 'hard'], ['hi', 'ken', 'always', 'video', 'insightfucking', 'fuck', 'much', 'fuck', 'helping'], ['watching', 'fucking', 'well', 'huge', 'geek', 'fanboi'], ['good', 'resume', 'template', 'use'], ['like', 'bayesian', 'probably'], ['hey', 'ken', 'great', 'introductory', 'video', 'thanks', 'small', 'project', 'involving', 'blockchain', 'fuckchain', 'data', 'using', 'beautifuck', 'blockchain', 'data', 'mean', 'also', 'ifucking', 'fucking', 'one', 'would', 'great', 'discus'], ['hey', 'ken', 'would', 'love', 'hear', 'opinion', 'fuck', 'good', 'program', 'fuckground', 'studied', 'quantitative', 'economics', 'undergrad', 'without', 'programming', 'experience', 'except', 'statistical', 'program', 'like', 'eviews', 'stata', 'spss', 'currently', 'internship', 'fuckills', 'using', 'power', 'excel', 'powerbi', 'sql', 'python', 'spark', 'matplotlib', 'panda', 'etc'], ['thank', 'sir'], ['great', 'shove', 'video', 'parent', 'fuck', 'plan', 'later', 'life', 'lmao'], ['sir', 'bangladesh', 'bachelor', 'math', 'go', 'data', 'science'], ['hey', 'ken', 'choose', 'bachelor', 'business', 'administration', 'major', 'finance', 'continue', 'data', 'science'], ['thank', 'amazing', 'series', 'really', 'helpful'], ['danger', 'robinson', 'always', 'money'], ['ignore', 'studio', 'column', 'df_model']]\n"
     ]
    }
   ],
   "source": [
    "pd.options.display.max_rows = 30\n",
    "# data processing for gensim\n",
    "t0 = time()\n",
    "stopwds = nltk.corpus.stopwords.words('english')\n",
    "stopwds.append('ha')  # add 'ha' to stopword list for removal\n",
    "stopwds.append('wa')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def tokenize_docs(docs):\n",
    "    tokened_docs = []\n",
    "    for doc in docs:\n",
    "        doc = doc.lower()\n",
    "        doc_tokens = [token.strip() for token in wtk.tokenize(doc)]\n",
    "        doc_tokens = [wnl.lemmatize(token) for token in doc_tokens if not token.isnumeric()]\n",
    "        doc_tokens = [token for token in doc_tokens if len(token) > 1]\n",
    "        doc_tokens = [token for token in doc_tokens if token not in stopwds]\n",
    "        doc_tokens = list(filter(None, doc_tokens))\n",
    "        if doc_tokens:\n",
    "            tokened_docs.append(doc_tokens)\n",
    "    return tokened_docs\n",
    "\n",
    "tokened_docs = tokenize_docs(textdf.Reviews)\n",
    "print(\"Computing time: %0.3f seconds.\" % (time() - t0))\n",
    "print('\\nTokenized documents:\\n', tokened_docs[:20])  # shows a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c345345c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "dictionary = Dictionary(tokened_docs)\n",
    "bow_tf = [dictionary.doc2bow(word) for word in tokened_docs]\n",
    "# print(dictionary)\n",
    "# print('\\n', list(dictionary.items())) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fec51ab",
   "metadata": {},
   "source": [
    "#### Gensim for LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ffbd3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LsiModel<num_terms=10232, num_topics=5, decay=1.0, chunksize=20000>\n"
     ]
    }
   ],
   "source": [
    "# gensim for latent semantic indexing (LSI)\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "NumberOfTopics = 5\n",
    "lsi = LsiModel(corpus=bow_tf, num_topics=NumberOfTopics, id2word=dictionary)\n",
    "print(lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7588359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0\n",
      "0.674*\"data\" + 0.413*\"science\" + 0.229*\"ken\" + 0.205*\"video\" + 0.197*\"fuck\" + 0.112*\"learning\" + 0.102*\"great\" + 0.097*\"would\" + 0.093*\"project\" + 0.091*\"scientist\" + 0.090*\"thanks\" + 0.082*\"get\" + 0.079*\"really\" + 0.079*\"job\" + 0.078*\"hi\"\n",
      "\n",
      "Topic 1\n",
      "0.445*\"video\" + -0.438*\"data\" + 0.418*\"ken\" + 0.334*\"fuck\" + -0.302*\"science\" + 0.224*\"great\" + 0.153*\"thanks\" + 0.123*\"really\" + 0.106*\"project\" + 0.100*\"thank\" + 0.095*\"hi\" + 0.079*\"fucke\" + 0.075*\"hey\" + 0.072*\"like\" + 0.067*\"much\"\n",
      "\n",
      "Topic 2\n",
      "0.660*\"video\" + -0.643*\"fuck\" + 0.245*\"great\" + -0.081*\"fucke\" + 0.070*\"science\" + -0.067*\"ofuck\" + 0.067*\"data\" + -0.061*\"hi\" + -0.059*\"project\" + -0.057*\"learning\" + -0.055*\"get\" + -0.051*\"much\" + -0.049*\"ken\" + -0.042*\"learn\" + -0.040*\"fucking\"\n",
      "\n",
      "Topic 3\n",
      "-0.773*\"ken\" + 0.357*\"fuck\" + 0.317*\"video\" + -0.170*\"hi\" + -0.136*\"hey\" + -0.113*\"thanks\" + -0.092*\"science\" + -0.087*\"jee\" + 0.082*\"learning\" + 0.080*\"really\" + 0.073*\"like\" + 0.073*\"great\" + 0.059*\"would\" + 0.058*\"ofuck\" + 0.056*\"project\"\n",
      "\n",
      "Topic 4\n",
      "-0.474*\"fuck\" + 0.469*\"project\" + 0.280*\"learning\" + -0.256*\"video\" + 0.192*\"really\" + 0.186*\"would\" + 0.172*\"one\" + 0.168*\"get\" + -0.167*\"data\" + 0.127*\"like\" + 0.105*\"course\" + 0.103*\"good\" + 0.100*\"machine\" + 0.098*\"fucke\" + 0.088*\"python\"\n"
     ]
    }
   ],
   "source": [
    "# print term-topic matrix (top 10 terms in each topic), the U matrix in X = U*S*V\n",
    "for i, topic in lsi.print_topics(num_words=15):\n",
    "    print('\\nTopic', i)\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e2b2069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "--------------------------------------------------\n",
      "Gropu +: [('data', 0.674), ('science', 0.413), ('ken', 0.229), ('video', 0.205), ('fuck', 0.197), ('learning', 0.112), ('great', 0.102), ('would', 0.097), ('project', 0.093), ('scientist', 0.091)]\n",
      "--------------------------------------------------\n",
      "Group -: []\n",
      "==================================================\n",
      "Topic 1\n",
      "--------------------------------------------------\n",
      "Gropu +: [('video', 0.445), ('ken', 0.418), ('fuck', 0.334), ('great', 0.224), ('thanks', 0.153), ('really', 0.123), ('project', 0.106), ('thank', 0.1)]\n",
      "--------------------------------------------------\n",
      "Group -: [('data', -0.438), ('science', -0.302)]\n",
      "==================================================\n",
      "Topic 2\n",
      "--------------------------------------------------\n",
      "Gropu +: [('video', 0.66), ('great', 0.245), ('science', 0.07), ('data', 0.067)]\n",
      "--------------------------------------------------\n",
      "Group -: [('fuck', -0.643), ('fucke', -0.081), ('ofuck', -0.067), ('hi', -0.061), ('project', -0.059), ('learning', -0.057)]\n",
      "==================================================\n",
      "Topic 3\n",
      "--------------------------------------------------\n",
      "Gropu +: [('fuck', 0.357), ('video', 0.317), ('learning', 0.082), ('really', 0.08)]\n",
      "--------------------------------------------------\n",
      "Group -: [('ken', -0.773), ('hi', -0.17), ('hey', -0.136), ('thanks', -0.113), ('science', -0.092), ('jee', -0.087)]\n",
      "==================================================\n",
      "Topic 4\n",
      "--------------------------------------------------\n",
      "Gropu +: [('project', 0.469), ('learning', 0.28), ('really', 0.192), ('would', 0.186), ('one', 0.172), ('get', 0.168), ('like', 0.127)]\n",
      "--------------------------------------------------\n",
      "Group -: [('fuck', -0.474), ('video', -0.256), ('data', -0.167)]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# seperate the terms with positive weight from those with negative weight\n",
    "for i in range(NumberOfTopics):\n",
    "    print('Topic', i)\n",
    "    print('-'*50)\n",
    "    g1 = []\n",
    "    g2 = []\n",
    "    for term, wt in lsi.show_topic(i, topn=10):\n",
    "        if wt >= 0: g1.append((term, round(wt, 3)))\n",
    "        else:       g2.append((term, round(wt, 3)))\n",
    "    print('Gropu +:', g1)\n",
    "    print('-'*50)\n",
    "    print('Group -:', g2)\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9655e671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10232, 5), (5,), (5, 10168))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print document-topic matrix, the V matrix in X = U*S*V\n",
    "from gensim.matutils import corpus2dense\n",
    "term_topic = lsi.projection.u  # left singular vectors\n",
    "singular_values = lsi.projection.s  # singular values\n",
    "topic_document = (corpus2dense(lsi[bow_tf], len(singular_values)).T / singular_values).T \n",
    "                             # lsi[bow_tf] is right singular vectors\n",
    "term_topic.shape, singular_values.shape, topic_document.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b690d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>-0.0030</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0363</td>\n",
       "      <td>-0.0053</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>-0.0138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>-0.0141</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>-0.0117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0218</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>0.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0018</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>0.0035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic 0  Topic 1  Topic 2  Topic 3  Topic 4\n",
       "0   0.0032   0.0040  -0.0030   0.0014   0.0127\n",
       "1   0.0363  -0.0053   0.0032  -0.0043  -0.0138\n",
       "2   0.0028   0.0077  -0.0141   0.0056  -0.0117\n",
       "3   0.0118   0.0218  -0.0010   0.0217   0.0078\n",
       "4   0.0013   0.0019  -0.0018   0.0019   0.0035"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics = pd.DataFrame(np.round(topic_document.T, 4),\n",
    "                               columns=['Topic '+str(i) for i in range(NumberOfTopics)])\n",
    "document_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810f705d",
   "metadata": {},
   "source": [
    "#### Gensim (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2861887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel<num_terms=10232, num_topics=5, decay=0.5, chunksize=2000>\n"
     ]
    }
   ],
   "source": [
    "# gensim for latent Dirichlet allocation (LDA)\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "lda_model = LdaModel(corpus=bow_tf, num_topics=NumberOfTopics, id2word=dictionary, random_state=1)\n",
    "print(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7f9a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print document-topic matrix, the theta parameter in LDA model\n",
    "topic_document_theta = corpus2dense(lda_model.get_document_topics(bow_tf), NumberOfTopics)\n",
    "document_topic_theta_df = pd.DataFrame(np.round(topic_document_theta.T, 4),\n",
    "                                       columns=['Topic '+str(i) for i in range(NumberOfTopics)])\n",
    "# document_topic_theta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96eb77bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average coherence score: -2.365187803749801\n"
     ]
    }
   ],
   "source": [
    "# get the topics with the highest coherence score based on top 10 terms\n",
    "topics_coherences = lda_model.top_topics(bow_tf, topn=10)  # default coherence='u_mass'\n",
    "\n",
    "# compute the average score for the top topics\n",
    "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
    "print('Average coherence score:', avg_coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "706f8cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_with_wts = [item[0] for item in topics_coherences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a87fb64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terms per Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>thanks, fuck, project, like, ken, thank, fucking, video, fucke, much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>data, science, learning, course, get, ken, master, scientist, hey, would</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>video, ken, great, thanks, fuck, hi, data, fucke, hey, good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>thank, tweet, ken, error, one, got, fuck, get, help, please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>video, thank, much, time, ken, fuck, great, channel, work, code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  Terms per Topic\n",
       "Topic 0      thanks, fuck, project, like, ken, thank, fucking, video, fucke, much\n",
       "Topic 1  data, science, learning, course, get, ken, master, scientist, hey, would\n",
       "Topic 2               video, ken, great, thanks, fuck, hi, data, fucke, hey, good\n",
       "Topic 3               thank, tweet, ken, error, one, got, fuck, get, help, please\n",
       "Topic 4           video, thank, much, time, ken, fuck, great, channel, work, code"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df = pd.DataFrame([[(term, round(wt, 3)) for (wt, term) in topic] for topic in topics_with_wts],\n",
    "                           columns=['Term '+str(i) for i in range(0, 10)],\n",
    "                           index=['Topic '+str(t) for t in range(0, lda_model.num_topics)]).T\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "topics_df = pd.DataFrame([', '.join([term for wt, term in topic]) for topic in topics_with_wts],\n",
    "                         columns=['Terms per Topic'],\n",
    "                         index=['Topic '+str(t) for t in range(0, 5)])\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28304052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average coherence score (c_v): 0.45430698132842046\n",
      "Average coherence score (u_mass): -2.9950420939880367\n",
      "Average coherence score (c_uci): -0.10625597414531388\n",
      "Average coherence score (c_npmi): 0.019166505933121046\n",
      "Model perplexity: -7.763722252439627\n"
     ]
    }
   ],
   "source": [
    "# compute four different coherence scores based on all terms\n",
    "from gensim.models import CoherenceModel\n",
    "cv_coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                        corpus=bow_tf, \n",
    "                                        texts=tokened_docs, \n",
    "                                        dictionary=dictionary, \n",
    "                                        coherence='c_v')\n",
    "print('Average coherence score (c_v):', cv_coherence_model_lda.get_coherence())\n",
    "\n",
    "umass_coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                           corpus=bow_tf,\n",
    "                                           texts=tokened_docs,\n",
    "                                           dictionary=dictionary,\n",
    "                                           coherence='u_mass')\n",
    "print('Average coherence score (u_mass):', umass_coherence_model_lda.get_coherence())\n",
    "\n",
    "uci_coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                         corpus=bow_tf,\n",
    "                                         texts=tokened_docs,\n",
    "                                         dictionary=dictionary,\n",
    "                                         coherence='c_uci')\n",
    "print('Average coherence score (c_uci):', uci_coherence_model_lda.get_coherence())\n",
    "\n",
    "npmi_coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                          corpus=bow_tf,\n",
    "                                          texts=tokened_docs,\n",
    "                                          dictionary=dictionary,\n",
    "                                          coherence='c_npmi')\n",
    "print('Average coherence score (c_npmi):', npmi_coherence_model_lda.get_coherence())\n",
    "\n",
    "perplexity = lda_model.log_perplexity(bow_tf)\n",
    "print('Model perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93761dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 19/19 [05:34<00:00, 17.60s/it]\n"
     ]
    }
   ],
   "source": [
    "# gensim tqdm search for the best number of topics for LDA based on coherence score\n",
    "from tqdm import tqdm\n",
    "def topic_model_coherence_generator(corpus, texts, dictionary,\n",
    "                                    start_topic_count, end_topic_count, step):\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for number_of_topics in tqdm(range(start_topic_count, end_topic_count+1, step)):\n",
    "        lda_model = LdaModel(corpus=bow_tf,\n",
    "                             num_topics=number_of_topics,\n",
    "                             id2word=dictionary,\n",
    "                             random_state=1)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                             corpus=bow_tf, \n",
    "                                             texts=tokened_docs, \n",
    "                                             dictionary=dictionary, \n",
    "                                             coherence='c_v')\n",
    "        coherence_score = coherence_model_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)\n",
    "        models.append(lda_model)\n",
    "    return models, coherence_scores\n",
    "\n",
    "lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_tf,\n",
    "                                                               texts=tokened_docs,\n",
    "                                                               dictionary=dictionary,\n",
    "                                                               start_topic_count=2,\n",
    "                                                               end_topic_count=20,\n",
    "                                                               step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db51b854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Topics</th>\n",
       "      <th>Coherence Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.4794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.4607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.4558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.4543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.4249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.4160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.4099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>0.4037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.3947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>0.3942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>0.3934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.3909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>0.3895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.3893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.3882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.3826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>0.3816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.3806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>0.3799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of Topics  Coherence Score\n",
       "2                  4           0.4794\n",
       "0                  2           0.4607\n",
       "1                  3           0.4558\n",
       "3                  5           0.4543\n",
       "5                  7           0.4249\n",
       "6                  8           0.4160\n",
       "4                  6           0.4099\n",
       "13                15           0.4037\n",
       "8                 10           0.3947\n",
       "15                17           0.3942\n",
       "17                19           0.3934\n",
       "10                12           0.3909\n",
       "14                16           0.3895\n",
       "9                 11           0.3893\n",
       "7                  9           0.3882\n",
       "12                14           0.3826\n",
       "16                18           0.3816\n",
       "11                13           0.3806\n",
       "18                20           0.3799"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the search results\n",
    "coherence_df = pd.DataFrame({'Number of Topics': range(2, 21, 1), #(start_topic_ct, end_topic_ct+1, step)\n",
    "                             'Coherence Score': np.round(coherence_scores, 4)})\n",
    "coherence_df.sort_values(by=['Coherence Score'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da3fe54",
   "metadata": {},
   "source": [
    "4 topics should be the best. But it seems not suitable for these dataset which are the comments for the same subject of data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49369dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "-end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

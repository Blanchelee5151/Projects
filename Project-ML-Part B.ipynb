{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2f482e",
   "metadata": {},
   "source": [
    "## Assignment 3                      -- Dan Hua Li\n",
    "1.\t(20 points) Perform latent semantic analysis (LSA), non-negative matrix factorization (NMF), and latent Dirichlet allocation (LDA) using scikit-learn and Gensim on the ***Amazon5000reviws dataset***. Set the number of topics = 5. Follow the same steps and parameter settings as those in the ***topic-modeling-animals-food-weather*** example (e.g., print out the top 10 words for each topic). Specific steps include:\n",
    "- a.\tAdd both ‘ha’ and ‘wa’ to the stop word list.\n",
    "- b.\tscikit-learn for LSA, NMF, and LDA.\n",
    "- c.\tscikit-learn grid search for the best number of topics k for LDA, based on the perplexity score, with the grid of k = [2,3,4,5,6,7,8,10,12,14,16,18,20] (this search may take about 10-20 minutes).\n",
    "- d.\tGensim for LSI, NMF, and LDA.\n",
    "- e.\tGensim tqdm search for the best number of topics k for LDA, based on the c_v coherence score, with k ranging from 2 to 20 (consecutively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b582993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Reviews  5000 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 39.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I thought it would be as big as small paper bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This kindle is light and easy to use especiall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Didnt know how much i'd use a kindle so went f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am 100 happy with my purchase. I caught it o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Solid entry level Kindle. Great for kids. Gift...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews\n",
       "0  I thought it would be as big as small paper bu...\n",
       "1  This kindle is light and easy to use especiall...\n",
       "2  Didnt know how much i'd use a kindle so went f...\n",
       "3  I am 100 happy with my purchase. I caught it o...\n",
       "4  Solid entry level Kindle. Great for kids. Gift..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install --upgrade gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "textdf = pd.read_csv('Amazon5000reviews.csv', sep=',')\n",
    "textdf.info()\n",
    "textdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b71c878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arent', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'ha', 'hadnt', 'hasnt', 'havent', 'isnt', 'mightnt', 'mustnt', 'neednt', 'shant', 'shes', 'shouldnt', 'shouldve', 'thatll', 'wa', 'wasnt', 'werent', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '10' ... '‚äúthings' '‚äúthings‚äù' '‚ù§ô∏è']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5000x5442 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 72870 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization with sklearn and nltk: set to lower case, remove stop words, and lemmatize words\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def lemma_tokenizer(corpus): # a method to lemmatize corpus\n",
    "    corpus = ''.join([ch for ch in corpus if ch not in string.punctuation]) # remove punctuation\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('english') # use nltk's English stopwords list\n",
    "tf = CountVectorizer(tokenizer=lemma_tokenizer, stop_words=nltk_stopwords) # default lowercase\n",
    "tf_sparse = tf.fit_transform(textdf.Reviews)\n",
    "tf_dictionary = tf.get_feature_names_out()\n",
    "print(tf_dictionary)\n",
    "tf_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b004cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arent', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'hadnt', 'hasnt', 'havent', 'isnt', 'mightnt', 'mustnt', 'neednt', 'shant', 'shes', 'shouldnt', 'shouldve', 'thatll', 'wasnt', 'werent', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '1' '10' ... '‚äúthings' '‚äúthings‚äù' '‚ù§ô∏è']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5000x5440 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 71557 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stopwords.append('ha') # add 'ha' to stopword list for removal\n",
    "nltk_stopwords.append('wa')\n",
    "tf = CountVectorizer(tokenizer=lemma_tokenizer, stop_words=nltk_stopwords) # default lowercase\n",
    "tf_sparse = tf.fit_transform(textdf.Reviews)\n",
    "tf_dictionary = tf.get_feature_names_out()\n",
    "print(tf_dictionary)\n",
    "tf_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4f9c2e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>101</th>\n",
       "      <th>1012</th>\n",
       "      <th>1013</th>\n",
       "      <th>105</th>\n",
       "      <th>1080</th>\n",
       "      <th>...</th>\n",
       "      <th>‚äúalexa‚äù</th>\n",
       "      <th>‚äúbest‚äù</th>\n",
       "      <th>‚äúdropping</th>\n",
       "      <th>‚äúdualbattery</th>\n",
       "      <th>‚äúshow‚äù</th>\n",
       "      <th>‚äúskills‚äù</th>\n",
       "      <th>‚äústar</th>\n",
       "      <th>‚äúthings</th>\n",
       "      <th>‚äúthings‚äù</th>\n",
       "      <th>‚ù§ô∏è</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 5440 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  10  100  1000  101  1012  1013  105  1080  ...  ‚äúalexa‚äù  \\\n",
       "0     0  0   0    0     0    0     0     0    0     0  ...            0   \n",
       "1     0  0   0    0     0    0     0     0    0     0  ...            0   \n",
       "2     0  0   0    0     0    0     0     0    0     0  ...            0   \n",
       "3     0  1   0    1     0    0     0     0    0     0  ...            0   \n",
       "4     0  0   0    0     0    0     0     0    0     0  ...            0   \n",
       "...  .. ..  ..  ...   ...  ...   ...   ...  ...   ...  ...          ...   \n",
       "4995  0  0   0    0     0    0     0     0    0     0  ...            0   \n",
       "4996  0  0   0    0     0    0     0     0    0     0  ...            0   \n",
       "4997  0  0   0    0     0    0     0     0    0     0  ...            0   \n",
       "4998  0  0   0    0     0    0     0     0    0     0  ...            0   \n",
       "4999  0  0   0    0     0    0     0     0    0     0  ...            0   \n",
       "\n",
       "      ‚äúbest‚äù  ‚äúdropping  ‚äúdualbattery  ‚äúshow‚äù  ‚äúskills‚äù  \\\n",
       "0              0            0               0           0             0   \n",
       "1              0            0               0           0             0   \n",
       "2              0            0               0           0             0   \n",
       "3              0            0               0           0             0   \n",
       "4              0            0               0           0             0   \n",
       "...          ...          ...             ...         ...           ...   \n",
       "4995           0            0               0           0             0   \n",
       "4996           0            0               0           0             0   \n",
       "4997           0            0               0           0             0   \n",
       "4998           0            0               0           0             0   \n",
       "4999           0            0               0           0             0   \n",
       "\n",
       "      ‚äústar  ‚äúthings  ‚äúthings‚äù  ‚ù§ô∏è  \n",
       "0           0          0             0       0  \n",
       "1           0          0             0       0  \n",
       "2           0          0             0       0  \n",
       "3           0          0             0       0  \n",
       "4           0          0             0       0  \n",
       "...       ...        ...           ...     ...  \n",
       "4995        0          0             0       0  \n",
       "4996        0          0             0       0  \n",
       "4997        0          0             0       0  \n",
       "4998        0          0             0       0  \n",
       "4999        0          0             0       0  \n",
       "\n",
       "[5000 rows x 5440 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dense = tf_sparse.toarray() # convert sparse to dense matrix\n",
    "pd.DataFrame(tf_dense, columns=tf_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a33cd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\yy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['arent', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'hadnt', 'hasnt', 'havent', 'isnt', 'mightnt', 'mustnt', 'neednt', 'shant', 'shes', 'shouldnt', 'shouldve', 'thatll', 'wasnt', 'werent', 'wont', 'wouldnt', 'youd', 'youll', 'youre', 'youve'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5000x5440 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 71557 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(tokenizer=lemma_tokenizer, stop_words=nltk_stopwords) # default lowercase\n",
    "tfidf_sparse = tfidf.fit_transform(textdf.Reviews)\n",
    "tfidf_dictionary = tfidf.get_feature_names_out()\n",
    "tfidf_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aa3270b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TruncatedSVD(n_components=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TruncatedSVD</label><div class=\"sk-toggleable__content\"><pre>TruncatedSVD(n_components=5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TruncatedSVD(n_components=5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=5)\n",
    "lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18db6458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_tf_topics = lsa.fit_transform(tf_sparse)\n",
    "lsa_tf_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0843dbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5440)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25610551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA topics based on term-document matrix:\n",
      "Topic #0: tablet device amazon use screen one great apps kindle love\n",
      "Topic #1: device magazine amazon app screen apps photo button menu home\n",
      "Topic #2: kindle charge oasis book read would cover one day reading\n",
      "Topic #3: echo show alexa music sound dot device video home see\n",
      "Topic #4: great tablet would echo work price good charge oasis sound\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print top terms for each topic\n",
    "def print_top_terms(model, vocabulary, n_top_terms):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([vocabulary[i]\n",
    "                             for i in topic.argsort()[:-n_top_terms - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "print('LSA topics based on term-document matrix:')\n",
    "print_top_terms(lsa, tf_dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff90329b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NMF(alpha_W=0.1, l1_ratio=0.5, n_components=5, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NMF</label><div class=\"sk-toggleable__content\"><pre>NMF(alpha_W=0.1, l1_ratio=0.5, n_components=5, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NMF(alpha_W=0.1, l1_ratio=0.5, n_components=5, random_state=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn for non-negative matrix factorization (NMF)\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=5, random_state=1, alpha_W=.1, l1_ratio=.5) # alpha_W and l1 related to regularization\n",
    "nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e88c196b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF topics based on term-document matrix:\n",
      "Topic #0: ‚ù§ô∏è exposed expirence explained explaining explanatory explore explored exploring exponentially\n",
      "Topic #1: ‚ù§ô∏è exposed expirence explained explaining explanatory explore explored exploring exponentially\n",
      "Topic #2: ‚ù§ô∏è exposed expirence explained explaining explanatory explore explored exploring exponentially\n",
      "Topic #3: ‚ù§ô∏è exposed expirence explained explaining explanatory explore explored exploring exponentially\n",
      "Topic #4: ‚ù§ô∏è exposed expirence explained explaining explanatory explore explored exploring exponentially\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf.fit_transform(tf_sparse)\n",
    "print('NMF topics based on term-document matrix:')\n",
    "print_top_terms(nmf, tf_dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "898312ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF topics based on tfidf matrix:\n",
      "Topic #0: ‚ù§ô∏è exposed expirence explained explaining explanatory explore explored exploring exponentially\n",
      "Topic #1: ‚ù§ô∏è exposed expirence explained explaining explanatory explore explored exploring exponentially\n",
      "Topic #2: ‚ù§ô∏è exposed expirence explained explaining explanatory explore explored exploring exponentially\n",
      "Topic #3: ‚ù§ô∏è exposed expirence explained explaining explanatory explore explored exploring exponentially\n",
      "Topic #4: ‚ù§ô∏è exposed expirence explained explaining explanatory explore explored exploring exponentially\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf.fit_transform(tfidf_sparse)\n",
    "print('NMF topics based on tfidf matrix:')\n",
    "print_top_terms(nmf, tfidf_dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0e10e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          n_components=5, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          n_components=5, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n",
       "                          n_components=5, random_state=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn for latent Dirichlet allocation (LDA)\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=5, random_state=1, learning_method='online', learning_offset=50.)\n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c652391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topics based on term-document matrix:\n",
      "Topic #0: music alexa love echo enjoy video play family question home\n",
      "Topic #1: tablet love great one use bought kindle easy kid fire\n",
      "Topic #2: kindle model user memory storage card page ereader voyage friendly\n",
      "Topic #3: great work echo product use show easy sound good recommend\n",
      "Topic #4: book read reader reading web email facebook ease shop want\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda.fit_transform(tf_sparse)\n",
    "print('LDA topics based on term-document matrix:')\n",
    "print_top_terms(lda, tf_dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49bc34d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topics based on tfidf matrix:\n",
      "Topic #0: great love tablet use easy bought good one kindle price\n",
      "Topic #1: better beginner good upgrade position could located allow kindle father\n",
      "Topic #2: keeping smooth asesome reallllllllllllllllllllllllllllllllllllllllllllllly travelling grandson8loves advertised ui pricereally active\n",
      "Topic #3: tableti 6yr author title recipient performed fundraiser competitive prize usewould\n",
      "Topic #4: class replaces nexus 3yr increase cousin font goodexcellent feachers screenlike\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda.fit_transform(tfidf_sparse)\n",
    "print('LDA topics based on tfidf matrix:')\n",
    "print_top_terms(lda, tfidf_dictionary, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d62555e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=LatentDirichletAllocation(learning_method=&#x27;online&#x27;,\n",
       "                                                 learning_offset=50.0,\n",
       "                                                 random_state=1),\n",
       "             param_grid={&#x27;n_components&#x27;: [2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 16,\n",
       "                                          18, 20]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=LatentDirichletAllocation(learning_method=&#x27;online&#x27;,\n",
       "                                                 learning_offset=50.0,\n",
       "                                                 random_state=1),\n",
       "             param_grid={&#x27;n_components&#x27;: [2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 16,\n",
       "                                          18, 20]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          random_state=1)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          random_state=1)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=LatentDirichletAllocation(learning_method='online',\n",
       "                                                 learning_offset=50.0,\n",
       "                                                 random_state=1),\n",
       "             param_grid={'n_components': [2, 3, 4, 5, 6, 7, 8, 10, 12, 14, 16,\n",
       "                                          18, 20]})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn grid search for the best number of topics for LDA based on perplexity score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_components': [2,3,4,5,6,7,8,10,12,14,16,18,20]}\n",
    "lda = LatentDirichletAllocation(random_state=1, learning_method='online', learning_offset=50.)\n",
    "lda_grid = GridSearchCV(lda, param_grid)\n",
    "lda_grid.fit(tf_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70ac3ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model's params:  {'n_components': 2}\n",
      "Best log likelihood score:  -121263.99937418592\n",
      "Model perplexity:  973.3272661707207\n",
      "-121263.99937418592 {'n_components': 2}\n",
      "-123541.91503155879 {'n_components': 3}\n",
      "-125219.1148578464 {'n_components': 4}\n",
      "-126932.75652981366 {'n_components': 5}\n",
      "-127402.07122772327 {'n_components': 6}\n",
      "-130354.38193482 {'n_components': 7}\n",
      "-129932.3043603483 {'n_components': 8}\n",
      "-133897.86389593122 {'n_components': 10}\n",
      "-134332.02489993474 {'n_components': 12}\n",
      "-136257.83153047334 {'n_components': 14}\n",
      "-137876.81880141242 {'n_components': 16}\n",
      "-140247.02924032547 {'n_components': 18}\n",
      "-142645.58511122147 {'n_components': 20}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best model's params: \", lda_grid.best_params_)\n",
    "print(\"Best log likelihood score: \", lda_grid.best_score_)\n",
    "print(\"Model perplexity: \", lda_grid.best_estimator_.perplexity(tf_sparse))\n",
    "cvresult = lda_grid.cv_results_\n",
    "for mean_test_score, params in zip(cvresult['mean_test_score'], cvresult['params']):\n",
    "    print(mean_test_score, params)\n",
    "# the best number of topics is 2 based on sklearn grid search, which does not looks good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4977899e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          n_components=2, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(learning_method=&#x27;online&#x27;, learning_offset=50.0,\n",
       "                          n_components=2, random_state=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n",
       "                          n_components=2, random_state=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=2, random_state=1, learning_method='online', learning_offset=50.)\n",
    "lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df8f82e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topics based on tfidf matrix:\n",
      "Topic #0: echo alexa music show sound home great family love speaker\n",
      "Topic #1: tablet great love easy use bought good kindle one price\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda.fit_transform(tfidf_sparse)\n",
    "print('LDA topics based on tfidf matrix:')\n",
    "print_top_terms(lda, tfidf_dictionary, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb450b6",
   "metadata": {},
   "source": [
    "#### Gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c967362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing for gensim\n",
    "t0 = time()\n",
    "stopwds = nltk.corpus.stopwords.words('english')\n",
    "stopwds.append('ha')  # add 'ha' to stopword list for removal\n",
    "stopwds.append('wa')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def tokenize_docs(docs):\n",
    "    tokened_docs = []\n",
    "    for doc in docs:\n",
    "        doc = doc.lower()\n",
    "        doc_tokens = [token.strip() for token in wtk.tokenize(doc)]\n",
    "        doc_tokens = [wnl.lemmatize(token) for token in doc_tokens if not token.isnumeric()]\n",
    "        doc_tokens = [token for token in doc_tokens if len(token) > 1]\n",
    "        doc_tokens = [token for token in doc_tokens if token not in stopwds]\n",
    "        doc_tokens = list(filter(None, doc_tokens))\n",
    "        if doc_tokens:\n",
    "            tokened_docs.append(doc_tokens)\n",
    "    return tokened_docs\n",
    "\n",
    "tokened_docs = tokenize_docs(textdf.Reviews)\n",
    "print(\"Computing time: %0.3f seconds.\" % (time() - t0))\n",
    "print('\\nTokenized documents:\\n', tokened_docs[:50])  # shows a little"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f91e0",
   "metadata": {},
   "source": [
    "### Output:>\n",
    "\n",
    "Computing time: 1.490 seconds.\n",
    "\n",
    "Tokenized documents:\n",
    " [['thought', 'would', 'big', 'small', 'paper', 'turn', 'like', 'palm', 'think', 'small', 'read', 'comfortable', 'regular', 'kindle', 'would', 'definitely', 'recommend', 'paperwhite', 'instead'], ['kindle', 'light', 'easy', 'use', 'especially', 'beach'], ['didnt', 'know', 'much', 'use', 'kindle', 'went', 'lower', 'end', 'im', 'happy', 'even', 'little', 'dark'], ['happy', 'purchase', 'caught', 'sale', 'really', 'good', 'price', 'normally', 'real', 'book', 'person', 'year', 'old', 'love', 'ripping', 'page', 'kindle', 'prevents', 'extremely', 'portable', 'fit', 'better', 'purse', 'giant', 'book', 'loaded', 'lot', 'book', 'finish', 'one', 'start', 'another', 'without', 'go', 'store', 'serf', 'need', 'picked', 'one', 'paperwhite', 'price', 'unbeatable', 'difference', 'could', 'see', 'one', 'backlit', 'simple', 'book', 'light', 'dollar', 'tree', 'solves', 'issue', 'second', 'kindle', 'first', 'old', 'keyboard', 'model', 'put', 'fell', 'love', 'keyboard', 'lol', 'likely', 'last'], ['solid', 'entry', 'level', 'kindle', 'great', 'kid', 'gifted', 'kid', 'friend', 'love', 'use', 'read', 'ipads', 'battery', 'good', 'higher', 'model', 'bit', 'better'], ['make', 'excellent', 'ebook', 'reader', 'expect', 'much', 'device', 'except', 'read', 'basic', 'ebooks', 'good', 'thing', 'cheap', 'good', 'read', 'sun'], ['ordered', 'daughter', 'black', 'paperwhite', 'love', 'read', 'quite', 'bit', 'larger', 'book', 'driving', 'crazy', 'hold', 'laying', 'wanting', 'take', 'book', 'vacation', 'lugging', 'around', 'thick', 'paperback', 'throw', 'bag', 'read', 'anywhere', 'light', 'weight', 'easy', 'use', 'batter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8178734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "dictionary = Dictionary(tokened_docs)\n",
    "print(dictionary)\n",
    "print('\\n', list(dictionary.items()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa303e0",
   "metadata": {},
   "source": [
    "#### Output: >\n",
    "\n",
    "Select some lists show belows:\n",
    "\n",
    "Dictionary<4650 unique tokens: ['big', 'comfortable', 'definitely', 'instead', 'kindle']...>\n",
    "\n",
    " [(0, 'big'), (1, 'comfortable'), (2, 'definitely'), (3, 'instead'), (4, 'kindle'), (5, 'like'), (6, 'palm'), (7, 'paper'), (8, 'paperwhite'), (9, 'read'), (10, 'recommend'), (11, 'regular'), (12, 'small'), (13, 'think'), (14, 'thought'), (15, 'turn'), (16, 'would'), (17, 'beach'), (18, 'easy'), (19, 'especially'), (20, 'light'), (21, 'use'), (22, 'dark'), (23, 'didnt'), (24, 'end'), (25, 'even'), (26, 'happy'), (27, 'im'), (28, 'know'), (29, 'little'), (30, 'lower'), (31, 'much'), (32, 'went'), (33, 'another'), (34, 'backlit'), (35, 'better'), (36, 'book'), (37, 'caught'), (38, 'could'), (39, 'difference'), (40, 'dollar'), (41, 'extremely'), (42, 'fell'), (43, 'finish'), (44, 'first'), (45, 'fit'), (46, 'giant'), (47, 'go'), (48, 'good'), (49, 'issue'), (50, 'keyboard'), (51, 'last'), (52, 'likely'), (53, 'loaded'), (54, 'lol'), (55, 'lot'), (56, 'love'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69de3f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_tf = [dictionary.doc2bow(word) for word in tokened_docs]  # bow for 'bag of words'\n",
    "print(bow_tf [:50])   # term-document matrix represented in (term_id, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e203b8b6",
   "metadata": {},
   "source": [
    "### Output:>\n",
    "\n",
    "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 2)], [(4, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(4, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)], [(4, 2), (8, 1), (20, 1), (26, 1), (33, 1), (34, 1), (35, 1), (36, 4), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 2), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 2), (57, 1), (58, 1), (59, 1), (60, 2), (61, 3), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 2), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1)], [(4, 1), (9, 1), (21, 1), (35, 1), (48, 1), (56, 1), (57, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 2), (95, 1), (96, 1)], [(9, 2), (31, 1), (48, 2), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1)], [(8, 1), (9, 3), (18, 1), (20, 1), (21, 1), (36, 4), (51, 1), (56, 1), (86, 1), (87, 1), (99, 2), (105, 1), (106, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 2), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 2), (121, 2), (122, 1), (123, 2), (124, 1), (125, 1), (126, 1), (127, 2), (128, 1), (129, 1), (130, 2), (131, 1), (132, 1), (133, 1), (134, 2), (135, 1), (136, 2), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1)], [(4, 1), (86, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1)], [(4, 1), (57, 1), (100, 1), (110, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1)], [(25, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1)], [(9, 1), (18, 1), (41, 1), (48, 1), (51, 1), (72, 2), (86, 1), (102, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 1), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5cd12c",
   "metadata": {},
   "source": [
    "### Gensim for LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96eb1dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LsiModel<num_terms=4650, num_topics=5, decay=1.0, chunksize=20000>\n"
     ]
    }
   ],
   "source": [
    "# gensim for latent semantic indexing (LSI)\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "NumberOfTopics = 5\n",
    "lsi = LsiModel(corpus=bow_tf, num_topics=NumberOfTopics, id2word=dictionary)\n",
    "print(lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68e5b166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0\n",
      "0.320*\"tablet\" + 0.290*\"device\" + 0.279*\"amazon\" + 0.233*\"screen\" + 0.220*\"use\" + 0.179*\"one\" + 0.169*\"apps\" + 0.157*\"great\" + 0.149*\"kindle\" + 0.135*\"love\"\n",
      "\n",
      "Topic 1\n",
      "0.303*\"kindle\" + -0.298*\"device\" + 0.287*\"love\" + 0.265*\"great\" + -0.191*\"magazine\" + 0.179*\"would\" + -0.172*\"amazon\" + 0.168*\"bought\" + -0.156*\"apps\" + -0.155*\"app\"\n",
      "\n",
      "Topic 2\n",
      "-0.407*\"kindle\" + 0.367*\"great\" + 0.274*\"echo\" + 0.215*\"love\" + -0.206*\"book\" + -0.200*\"charge\" + 0.187*\"tablet\" + -0.181*\"read\" + -0.181*\"oasis\" + -0.136*\"äôt\"\n",
      "\n",
      "Topic 3\n",
      "0.621*\"tablet\" + -0.473*\"echo\" + -0.262*\"show\" + -0.190*\"alexa\" + -0.140*\"music\" + 0.130*\"great\" + 0.119*\"kid\" + -0.117*\"sound\" + -0.116*\"device\" + 0.097*\"price\"\n",
      "\n",
      "Topic 4\n",
      "0.745*\"love\" + -0.393*\"great\" + -0.188*\"tablet\" + 0.147*\"old\" + 0.134*\"year\" + 0.133*\"bought\" + -0.125*\"echo\" + -0.124*\"would\" + -0.100*\"price\" + -0.099*\"work\"\n"
     ]
    }
   ],
   "source": [
    "# print term-topic matrix (top 10 terms in each topic), the U matrix in X = U*S*V\n",
    "for i, topic in lsi.print_topics(num_words=10):\n",
    "    print('\\nTopic', i)\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e76d9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "--------------------------------------------------\n",
      "Gropu +: [('tablet', 0.32), ('device', 0.29), ('amazon', 0.279), ('screen', 0.233), ('use', 0.22), ('one', 0.179), ('apps', 0.169), ('great', 0.157), ('kindle', 0.149), ('love', 0.135)]\n",
      "--------------------------------------------------\n",
      "Group -: []\n",
      "==================================================\n",
      "Topic 1\n",
      "--------------------------------------------------\n",
      "Gropu +: [('kindle', 0.303), ('love', 0.287), ('great', 0.265), ('would', 0.179), ('bought', 0.168)]\n",
      "--------------------------------------------------\n",
      "Group -: [('device', -0.298), ('magazine', -0.191), ('amazon', -0.172), ('apps', -0.156), ('app', -0.155)]\n",
      "==================================================\n",
      "Topic 2\n",
      "--------------------------------------------------\n",
      "Gropu +: [('great', 0.367), ('echo', 0.274), ('love', 0.215), ('tablet', 0.187)]\n",
      "--------------------------------------------------\n",
      "Group -: [('kindle', -0.407), ('book', -0.206), ('charge', -0.2), ('read', -0.181), ('oasis', -0.181), ('äôt', -0.136)]\n",
      "==================================================\n",
      "Topic 3\n",
      "--------------------------------------------------\n",
      "Gropu +: [('tablet', 0.621), ('great', 0.13), ('kid', 0.119), ('price', 0.097)]\n",
      "--------------------------------------------------\n",
      "Group -: [('echo', -0.473), ('show', -0.262), ('alexa', -0.19), ('music', -0.14), ('sound', -0.117), ('device', -0.116)]\n",
      "==================================================\n",
      "Topic 4\n",
      "--------------------------------------------------\n",
      "Gropu +: [('love', 0.745), ('old', 0.147), ('year', 0.134), ('bought', 0.133)]\n",
      "--------------------------------------------------\n",
      "Group -: [('great', -0.393), ('tablet', -0.188), ('echo', -0.125), ('would', -0.124), ('price', -0.1), ('work', -0.099)]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# seperate the terms with positive weight from those with negative weight\n",
    "for i in range(NumberOfTopics):\n",
    "    print('Topic', i)\n",
    "    print('-'*50)\n",
    "    g1 = []\n",
    "    g2 = []\n",
    "    for term, wt in lsi.show_topic(i, topn=10):\n",
    "        if wt >= 0: g1.append((term, round(wt, 3)))\n",
    "        else:       g2.append((term, round(wt, 3)))\n",
    "    print('Gropu +:', g1)\n",
    "    print('-'*50)\n",
    "    print('Group -:', g2)\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b75092b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4650, 5), (5,), (5, 5000))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print document-topic matrix, the V matrix in X = U*S*V\n",
    "from gensim.matutils import corpus2dense\n",
    "term_topic = lsi.projection.u  # left singular vectors\n",
    "singular_values = lsi.projection.s  # singular values\n",
    "topic_document = (corpus2dense(lsi[bow_tf], len(singular_values)).T / singular_values).T \n",
    "                             # lsi[bow_tf] is right singular vectors\n",
    "term_topic.shape, singular_values.shape, topic_document.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31a25f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>-0.0171</td>\n",
       "      <td>-0.0051</td>\n",
       "      <td>-0.0084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>-0.0071</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>-0.0068</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0287</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>-0.0307</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic 0  Topic 1  Topic 2  Topic 3  Topic 4\n",
       "0   0.0074   0.0148  -0.0171  -0.0051  -0.0084\n",
       "1   0.0051   0.0085  -0.0071   0.0003   0.0039\n",
       "2   0.0056   0.0064  -0.0068   0.0004   0.0031\n",
       "3   0.0287   0.0399  -0.0307   0.0063   0.0410\n",
       "4   0.0105   0.0207   0.0036   0.0091   0.0083"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics = pd.DataFrame(np.round(topic_document.T, 4),\n",
    "                               columns=['Topic '+str(i) for i in range(NumberOfTopics)])\n",
    "document_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a810c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62b13cff",
   "metadata": {},
   "source": [
    "### Gensim for NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5374d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim for non-negative matrix factorization (NMF)\n",
    "from gensim.models.nmf import Nmf\n",
    "nmf_model = Nmf(corpus=bow_tf, num_topics=NumberOfTopics, id2word=dictionary, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f773e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0\n",
      "0.072*\"tablet\" + 0.063*\"great\" + 0.017*\"price\" + 0.016*\"work\" + 0.014*\"amazon\" + 0.013*\"game\" + 0.012*\"use\" + 0.011*\"kid\" + 0.010*\"play\" + 0.009*\"good\"\n",
      "\n",
      "Topic 1\n",
      "0.057*\"love\" + 0.019*\"bought\" + 0.018*\"year\" + 0.017*\"old\" + 0.015*\"use\" + 0.015*\"time\" + 0.013*\"like\" + 0.013*\"kid\" + 0.012*\"one\" + 0.011*\"tablet\"\n",
      "\n",
      "Topic 2\n",
      "0.030*\"device\" + 0.025*\"amazon\" + 0.019*\"screen\" + 0.017*\"magazine\" + 0.016*\"app\" + 0.014*\"apps\" + 0.014*\"alexa\" + 0.014*\"home\" + 0.010*\"photo\" + 0.009*\"show\"\n",
      "\n",
      "Topic 3\n",
      "0.031*\"one\" + 0.027*\"echo\" + 0.019*\"screen\" + 0.018*\"sound\" + 0.014*\"device\" + 0.013*\"use\" + 0.012*\"show\" + 0.012*\"great\" + 0.012*\"light\" + 0.011*\"see\"\n",
      "\n",
      "Topic 4\n",
      "0.044*\"kindle\" + 0.019*\"book\" + 0.018*\"read\" + 0.016*\"charge\" + 0.016*\"would\" + 0.015*\"oasis\" + 0.012*\"äôt\" + 0.011*\"cover\" + 0.009*\"day\" + 0.009*\"amazon\"\n"
     ]
    }
   ],
   "source": [
    "# print term-topic matrix (top 10 terms in each topic), the W matrix in X = W*H\n",
    "# all coefficients are non-negative\n",
    "for i, topic in nmf_model.print_topics(num_words=10):\n",
    "    print('\\nTopic', i)\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec89853a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1706</td>\n",
       "      <td>0.8294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3129</td>\n",
       "      <td>0.6871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1585</td>\n",
       "      <td>0.6045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4241</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1731</td>\n",
       "      <td>0.4028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.4564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic 0  Topic 1  Topic 2  Topic 3  Topic 4\n",
       "0   0.0000   0.0000      0.0   0.1706   0.8294\n",
       "1   0.0000   0.0000      0.0   0.3129   0.6871\n",
       "2   0.0000   0.2370      0.0   0.1585   0.6045\n",
       "3   0.0000   0.4241      0.0   0.1731   0.4028\n",
       "4   0.2164   0.4564      0.0   0.0000   0.3272"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print document-topic matrix, the H matrix (transpose) in X = W*H\n",
    "topic_documet_H = corpus2dense(nmf_model.get_document_topics(bow_tf), NumberOfTopics)\n",
    "document_topic_H_df = pd.DataFrame(np.round(topic_documet_H.T, 4),\n",
    "                                   columns=['Topic '+str(i) for i in range(NumberOfTopics)])\n",
    "document_topic_H_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c386ec",
   "metadata": {},
   "source": [
    "#### Gensim (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65ce8b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel<num_terms=4650, num_topics=5, decay=0.5, chunksize=2000>\n"
     ]
    }
   ],
   "source": [
    "# gensim for latent Dirichlet allocation (LDA)\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "lda_model = LdaModel(corpus=bow_tf, num_topics=NumberOfTopics, id2word=dictionary, random_state=1)\n",
    "print(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88417be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0\n",
      "0.023*\"kindle\" + 0.014*\"tablet\" + 0.013*\"year\" + 0.013*\"book\" + 0.013*\"old\" + 0.012*\"love\" + 0.012*\"bought\" + 0.010*\"use\" + 0.009*\"game\" + 0.009*\"amazon\"\n",
      "\n",
      "Topic 1\n",
      "0.039*\"tablet\" + 0.033*\"great\" + 0.017*\"use\" + 0.016*\"one\" + 0.015*\"amazon\" + 0.015*\"work\" + 0.014*\"good\" + 0.011*\"apps\" + 0.011*\"easy\" + 0.011*\"price\"\n",
      "\n",
      "Topic 2\n",
      "0.035*\"use\" + 0.029*\"tablet\" + 0.026*\"easy\" + 0.012*\"like\" + 0.012*\"good\" + 0.012*\"love\" + 0.011*\"book\" + 0.010*\"reading\" + 0.008*\"great\" + 0.008*\"device\"\n",
      "\n",
      "Topic 3\n",
      "0.047*\"love\" + 0.033*\"great\" + 0.024*\"kindle\" + 0.024*\"tablet\" + 0.018*\"one\" + 0.018*\"bought\" + 0.014*\"price\" + 0.012*\"fire\" + 0.012*\"kid\" + 0.012*\"book\"\n",
      "\n",
      "Topic 4\n",
      "0.016*\"screen\" + 0.015*\"like\" + 0.013*\"better\" + 0.011*\"would\" + 0.008*\"price\" + 0.008*\"one\" + 0.008*\"echo\" + 0.007*\"doe\" + 0.007*\"get\" + 0.007*\"fire\"\n"
     ]
    }
   ],
   "source": [
    "# print term-topic matrix (top 10 terms in each topic)\n",
    "for i, topic in lda_model.print_topics(num_words=10):\n",
    "    print('\\nTopic', i)\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9561e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.9590</td>\n",
       "      <td>0.0104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.8831</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>0.0291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7667</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.1888</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>0.0149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3309</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.4955</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4730</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.2764</td>\n",
       "      <td>0.2301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic 0  Topic 1  Topic 2  Topic 3  Topic 4\n",
       "0   0.0102   0.0102   0.0102   0.9590   0.0104\n",
       "1   0.0293   0.0289   0.8831   0.0296   0.0291\n",
       "2   0.7667   0.0148   0.1888   0.0147   0.0149\n",
       "3   0.3309   0.0000   0.1674   0.4955   0.0000\n",
       "4   0.4730   0.0102   0.0102   0.2764   0.2301"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print document-topic matrix, the theta parameter in LDA model\n",
    "topic_document_theta = corpus2dense(lda_model.get_document_topics(bow_tf), NumberOfTopics)\n",
    "document_topic_theta_df = pd.DataFrame(np.round(topic_document_theta.T, 4),\n",
    "                                       columns=['Topic '+str(i) for i in range(NumberOfTopics)])\n",
    "document_topic_theta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13787a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average coherence score: -2.028441029653849\n"
     ]
    }
   ],
   "source": [
    "# get the topics with the highest coherence score based on top 10 terms\n",
    "topics_coherences = lda_model.top_topics(bow_tf, topn=10)  # default coherence='u_mass'\n",
    "\n",
    "# compute the average score for the top topics\n",
    "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
    "print('Average coherence score:', avg_coherence_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c3148",
   "metadata": {},
   "source": [
    "#### show topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61cbf899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA topics with weights:\n",
      "\n",
      "Topic 0\n",
      "[('kindle', 0.023), ('tablet', 0.014), ('year', 0.013), ('book', 0.013), ('old', 0.013), ('love', 0.012), ('bought', 0.012), ('use', 0.01), ('game', 0.009), ('amazon', 0.009)]\n",
      "\n",
      "Topic 1\n",
      "[('tablet', 0.039), ('great', 0.033), ('use', 0.017), ('one', 0.016), ('amazon', 0.015), ('work', 0.015), ('good', 0.014), ('apps', 0.011), ('easy', 0.011), ('price', 0.011)]\n",
      "\n",
      "Topic 2\n",
      "[('use', 0.035), ('tablet', 0.029), ('easy', 0.026), ('like', 0.012), ('good', 0.012), ('love', 0.012), ('book', 0.011), ('reading', 0.01), ('great', 0.008), ('device', 0.008)]\n",
      "\n",
      "Topic 3\n",
      "[('love', 0.047), ('great', 0.033), ('kindle', 0.024), ('tablet', 0.024), ('one', 0.018), ('bought', 0.018), ('price', 0.014), ('fire', 0.012), ('kid', 0.012), ('book', 0.012)]\n",
      "\n",
      "Topic 4\n",
      "[('screen', 0.016), ('like', 0.015), ('better', 0.013), ('would', 0.011), ('price', 0.008), ('one', 0.008), ('echo', 0.008), ('doe', 0.007), ('get', 0.007), ('fire', 0.007)]\n"
     ]
    }
   ],
   "source": [
    "# other ways to show the results with top 10 terms\n",
    "topics_with_wts = [item[0] for item in topics_coherences]\n",
    "print('LDA topics with weights:')\n",
    "for i, topic in enumerate(topics_with_wts):\n",
    "    print('\\nTopic', i)\n",
    "    print([(term, round(wt, 3)) for wt, term in topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00e390da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terms per Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>kindle, tablet, year, book, old, love, bought, use, game, amazon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>tablet, great, use, one, amazon, work, good, apps, easy, price</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>use, tablet, easy, like, good, love, book, reading, great, device</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>love, great, kindle, tablet, one, bought, price, fire, kid, book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>screen, like, better, would, price, one, echo, doe, get, fire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           Terms per Topic\n",
       "Topic 0   kindle, tablet, year, book, old, love, bought, use, game, amazon\n",
       "Topic 1     tablet, great, use, one, amazon, work, good, apps, easy, price\n",
       "Topic 2  use, tablet, easy, like, good, love, book, reading, great, device\n",
       "Topic 3   love, great, kindle, tablet, one, bought, price, fire, kid, book\n",
       "Topic 4      screen, like, better, would, price, one, echo, doe, get, fire"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df = pd.DataFrame([[(term, round(wt, 3)) for (wt, term) in topic] for topic in topics_with_wts],\n",
    "                           columns=['Term '+str(i) for i in range(0, 10)],\n",
    "                           index=['Topic '+str(t) for t in range(0, lda_model.num_topics)]).T\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "topics_df = pd.DataFrame([', '.join([term for wt, term in topic]) for topic in topics_with_wts],\n",
    "                         columns=['Terms per Topic'],\n",
    "                         index=['Topic '+str(t) for t in range(0, 5)])\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81e94946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average coherence score (c_v): 0.31110491219621467\n",
      "Average coherence score (u_mass): -2.312433545281084\n",
      "Average coherence score (c_uci): -0.0651867048688011\n",
      "Average coherence score (c_npmi): 0.0009089531016212293\n",
      "Model perplexity: -7.075235269703249\n"
     ]
    }
   ],
   "source": [
    "# compute four different coherence scores based on all terms\n",
    "from gensim.models import CoherenceModel\n",
    "cv_coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                        corpus=bow_tf, \n",
    "                                        texts=tokened_docs, \n",
    "                                        dictionary=dictionary, \n",
    "                                        coherence='c_v')\n",
    "print('Average coherence score (c_v):', cv_coherence_model_lda.get_coherence())\n",
    "\n",
    "umass_coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                           corpus=bow_tf,\n",
    "                                           texts=tokened_docs,\n",
    "                                           dictionary=dictionary,\n",
    "                                           coherence='u_mass')\n",
    "print('Average coherence score (u_mass):', umass_coherence_model_lda.get_coherence())\n",
    "\n",
    "uci_coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                         corpus=bow_tf,\n",
    "                                         texts=tokened_docs,\n",
    "                                         dictionary=dictionary,\n",
    "                                         coherence='c_uci')\n",
    "print('Average coherence score (c_uci):', uci_coherence_model_lda.get_coherence())\n",
    "\n",
    "npmi_coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                          corpus=bow_tf,\n",
    "                                          texts=tokened_docs,\n",
    "                                          dictionary=dictionary,\n",
    "                                          coherence='c_npmi')\n",
    "print('Average coherence score (c_npmi):', npmi_coherence_model_lda.get_coherence())\n",
    "\n",
    "perplexity = lda_model.log_perplexity(bow_tf)\n",
    "print('Model perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f3b5b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 19/19 [04:40<00:00, 14.75s/it]\n"
     ]
    }
   ],
   "source": [
    "# gensim tqdm search for the best number of topics for LDA based on coherence score\n",
    "from tqdm import tqdm\n",
    "def topic_model_coherence_generator(corpus, texts, dictionary,\n",
    "                                    start_topic_count, end_topic_count, step):\n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for number_of_topics in tqdm(range(start_topic_count, end_topic_count+1, step)):\n",
    "        lda_model = LdaModel(corpus=bow_tf,\n",
    "                             num_topics=number_of_topics,\n",
    "                             id2word=dictionary,\n",
    "                             random_state=1)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model,\n",
    "                                             corpus=bow_tf, \n",
    "                                             texts=tokened_docs, \n",
    "                                             dictionary=dictionary, \n",
    "                                             coherence='c_v')\n",
    "        coherence_score = coherence_model_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)\n",
    "        models.append(lda_model)\n",
    "    return models, coherence_scores\n",
    "\n",
    "lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_tf,\n",
    "                                                               texts=tokened_docs,\n",
    "                                                               dictionary=dictionary,\n",
    "                                                               start_topic_count=2,\n",
    "                                                               end_topic_count=20,\n",
    "                                                               step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20a0adcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Topics</th>\n",
       "      <th>Coherence Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.3164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.3160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.3158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.3148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.3135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.3112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.3111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>0.3087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.3063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>0.3032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.2996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.2981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.2959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.2955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>0.2951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>0.2916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>0.2915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>0.2837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.2804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of Topics  Coherence Score\n",
       "9                 11           0.3164\n",
       "6                  8           0.3160\n",
       "5                  7           0.3158\n",
       "12                14           0.3148\n",
       "8                 10           0.3135\n",
       "10                12           0.3112\n",
       "3                  5           0.3111\n",
       "15                17           0.3087\n",
       "1                  3           0.3063\n",
       "13                15           0.3032\n",
       "7                  9           0.2996\n",
       "11                13           0.2981\n",
       "4                  6           0.2959\n",
       "2                  4           0.2955\n",
       "16                18           0.2951\n",
       "14                16           0.2916\n",
       "18                20           0.2915\n",
       "17                19           0.2837\n",
       "0                  2           0.2804"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the search results\n",
    "coherence_df = pd.DataFrame({'Number of Topics': range(2, 21, 1), #(start_topic_ct, end_topic_ct+1, step)\n",
    "                             'Coherence Score': np.round(coherence_scores, 4)})\n",
    "coherence_df.sort_values(by=['Coherence Score'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f6dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "-end- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
